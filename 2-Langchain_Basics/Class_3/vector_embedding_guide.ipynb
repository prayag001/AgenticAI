{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60abc4e2",
   "metadata": {},
   "source": [
    "# Vector Embeddings: From Zero to Hero with Python & LangChain\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Vector Embeddings](#introduction)\n",
    "2. [Understanding Vector Spaces](#vector-spaces)\n",
    "3. [Types of Embeddings](#types-of-embeddings)\n",
    "4. [Setting Up the Environment](#setup)\n",
    "5. [Basic Embeddings with Python](#basic-embeddings)\n",
    "6. [Working with LangChain Embeddings](#langchain-embeddings)\n",
    "7. [Vector Stores and Similarity Search](#vector-stores)\n",
    "8. [Building a RAG System](#rag-system)\n",
    "9. [Advanced Techniques](#advanced-techniques)\n",
    "10. [Best Practices](#best-practices)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Vector Embeddings {#introduction}\n",
    "\n",
    "Vector embeddings are numerical representations of text, images, or other data types that capture semantic meaning in a high-dimensional space. They enable machines to understand and process human language by converting words, sentences, or documents into vectors of real numbers.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Semantic Similarity**: Similar concepts are placed closer together in vector space\n",
    "- **Dimensionality**: Typical embeddings range from 128 to 4096 dimensions\n",
    "- **Dense Representations**: Unlike sparse one-hot encodings, embeddings are dense vectors\n",
    "\n",
    "### Why Vector Embeddings Matter:\n",
    "- Enable semantic search beyond keyword matching\n",
    "- Power recommendation systems\n",
    "- Essential for modern NLP applications\n",
    "- Foundation for RAG (Retrieval Augmented Generation) systems\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Understanding Vector Spaces {#vector-spaces}\n",
    "\n",
    "Vector embeddings exist in high-dimensional spaces where:\n",
    "- Each dimension represents a learned feature\n",
    "- Distance between vectors indicates semantic similarity\n",
    "- Common distance metrics: Cosine similarity, Euclidean distance, Dot product\n",
    "\n",
    "### Mathematical Foundation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74083a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Example: Simple word vectors\n",
    "word_vectors = {\n",
    "    'king': np.array([0.2, 0.8, 0.1, 0.9]),\n",
    "    'queen': np.array([0.3, 0.7, 0.2, 0.8]),\n",
    "    'man': np.array([0.1, 0.9, 0.05, 0.95]),\n",
    "    'woman': np.array([0.2, 0.8, 0.15, 0.85])\n",
    "}\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - cosine(v1, v2)\n",
    "\n",
    "# Example: king - man + woman â‰ˆ queen\n",
    "result = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n",
    "similarity = cosine_similarity(result, word_vectors['queen'])\n",
    "print(f\"Similarity between result and 'queen': {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c04a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Types of Embeddings {#types-of-embeddings}\n",
    "\n",
    "### 3.1 Word Embeddings\n",
    "- **Word2Vec**: Skip-gram and CBOW models\n",
    "- **GloVe**: Global Vectors for Word Representation\n",
    "- **FastText**: Subword information inclusion\n",
    "\n",
    "### 3.2 Sentence/Document Embeddings\n",
    "- **Sentence-BERT**: Bidirectional sentence representations\n",
    "- **Universal Sentence Encoder**: Google's sentence embedding model\n",
    "- **Doc2Vec**: Document-level embeddings\n",
    "\n",
    "### 3.3 Modern Transformer-based Embeddings\n",
    "- **BERT**: Bidirectional Encoder Representations\n",
    "- **RoBERTa**: Robustly Optimized BERT Pretraining\n",
    "- **OpenAI Embeddings**: GPT-based embedding models\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Setting Up the Environment {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install required packages\n",
    "pip install langchain\n",
    "pip install langchain-openai\n",
    "pip install langchain-community\n",
    "pip install chromadb\n",
    "pip install sentence-transformers\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Set up API keys (replace with your actual keys)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc854cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Basic Embeddings with Python {#basic-embeddings}\n",
    "\n",
    "### 5.1 Creating Simple Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"A feline rests on a rug\", \n",
    "    \"Dogs love to play fetch\",\n",
    "    \"Canines enjoy retrieving balls\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"AI algorithms are interesting\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"First embedding: {embeddings[0][:5]}...\")  # Show first 5 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55080626",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d660d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "\n",
    "# Add labels\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.annotate(sentence[:20] + \"...\", \n",
    "                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('2D Visualization of Sentence Embeddings')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc262af6",
   "metadata": {},
   "source": [
    "### 5.3 Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509aa542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            xticklabels=[s[:15] + \"...\" for s in sentences],\n",
    "            yticklabels=[s[:15] + \"...\" for s in sentences])\n",
    "plt.title('Cosine Similarity Between Sentences')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811d88a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Working with LangChain Embeddings {#langchain-embeddings}\n",
    "\n",
    "### 6.1 OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Generate embeddings for documents\n",
    "documents = [\n",
    "    \"LangChain is a framework for developing applications powered by language models.\",\n",
    "    \"Vector databases store and retrieve high-dimensional vectors efficiently.\",\n",
    "    \"Retrieval Augmented Generation combines retrieval with generation for better answers.\",\n",
    "    \"Embeddings capture semantic meaning in numerical form.\"\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "doc_embeddings = openai_embeddings.embed_documents(documents)\n",
    "print(f\"Number of documents: {len(doc_embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(doc_embeddings[0])}\")\n",
    "\n",
    "# Query embedding\n",
    "query = \"What is LangChain used for?\"\n",
    "query_embedding = openai_embeddings.embed_query(query)\n",
    "print(f\"Query embedding dimension: {len(query_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e6d69",
   "metadata": {},
   "source": [
    "### 6.2 HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize HuggingFace embeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Generate embeddings\n",
    "hf_doc_embeddings = hf_embeddings.embed_documents(documents)\n",
    "hf_query_embedding = hf_embeddings.embed_query(query)\n",
    "\n",
    "print(f\"HuggingFace embedding dimension: {len(hf_doc_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0904e25",
   "metadata": {},
   "source": [
    "### 6.3 Comparing Different Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(documents, query, embeddings_list, model_names):\n",
    "    \"\"\"Compare different embedding models\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for embeddings, name in zip(embeddings_list, model_names):\n",
    "        # Get document and query embeddings\n",
    "        doc_emb = embeddings.embed_documents(documents)\n",
    "        query_emb = embeddings.embed_query(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for doc_embedding in doc_emb:\n",
    "            similarity = cosine_similarity([query_emb], [doc_embedding])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        results[name] = similarities\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models\n",
    "embedding_models = [openai_embeddings, hf_embeddings]\n",
    "model_names = [\"OpenAI\", \"HuggingFace\"]\n",
    "\n",
    "comparison_results = compare_embeddings(documents, query, embedding_models, model_names)\n",
    "\n",
    "# Visualize comparison\n",
    "df = pd.DataFrame(comparison_results, index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
    "df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Similarity Scores: Different Embedding Models')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5a4dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Vector Stores and Similarity Search {#vector-stores}\n",
    "\n",
    "### 7.1 ChromaDB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b40763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Sample documents\n",
    "sample_texts = [\n",
    "    \"Vector embeddings are numerical representations of text that capture semantic meaning.\",\n",
    "    \"LangChain provides tools for building applications with large language models.\",\n",
    "    \"Retrieval Augmented Generation improves LLM responses by incorporating relevant context.\",\n",
    "    \"Vector databases enable efficient storage and retrieval of high-dimensional vectors.\",\n",
    "    \"Similarity search finds the most relevant documents based on semantic similarity.\",\n",
    "    \"Natural Language Processing has been revolutionized by transformer models.\",\n",
    "    \"Machine learning algorithms can learn patterns from large datasets.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to process information.\"\n",
    "]\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=sample_texts,\n",
    "    embedding=hf_embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"How do vector embeddings work?\"\n",
    "relevant_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nMost relevant documents:\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a6dfb",
   "metadata": {},
   "source": [
    "### 7.2 FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0142a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create FAISS vector store\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    texts=sample_texts,\n",
    "    embedding=hf_embeddings\n",
    ")\n",
    "\n",
    "# Similarity search with scores\n",
    "docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(\"FAISS Similarity Search Results:\")\n",
    "for doc, score in docs_with_scores:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3b30e",
   "metadata": {},
   "source": [
    "### 7.3 Advanced Vector Store Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more documents\n",
    "new_texts = [\n",
    "    \"Prompt engineering is crucial for getting good results from language models.\",\n",
    "    \"Fine-tuning adapts pre-trained models to specific tasks or domains.\",\n",
    "    \"Few-shot learning enables models to learn from just a few examples.\"\n",
    "]\n",
    "\n",
    "vectorstore.add_texts(new_texts)\n",
    "\n",
    "# Search with metadata filtering\n",
    "vectorstore_with_metadata = Chroma.from_texts(\n",
    "    texts=sample_texts + new_texts,\n",
    "    embedding=hf_embeddings,\n",
    "    metadatas=[\n",
    "        {\"category\": \"embeddings\", \"difficulty\": \"beginner\"},\n",
    "        {\"category\": \"langchain\", \"difficulty\": \"beginner\"},\n",
    "        {\"category\": \"rag\", \"difficulty\": \"intermediate\"},\n",
    "        {\"category\": \"vectordb\", \"difficulty\": \"intermediate\"},\n",
    "        {\"category\": \"search\", \"difficulty\": \"beginner\"},\n",
    "        {\"category\": \"nlp\", \"difficulty\": \"advanced\"},\n",
    "        {\"category\": \"ml\", \"difficulty\": \"intermediate\"},\n",
    "        {\"category\": \"dl\", \"difficulty\": \"advanced\"},\n",
    "        {\"category\": \"prompt\", \"difficulty\": \"intermediate\"},\n",
    "        {\"category\": \"finetune\", \"difficulty\": \"advanced\"},\n",
    "        {\"category\": \"fewshot\", \"difficulty\": \"advanced\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Search with metadata filter\n",
    "filtered_docs = vectorstore_with_metadata.similarity_search(\n",
    "    query,\n",
    "    k=3,\n",
    "    filter={\"difficulty\": \"beginner\"}\n",
    ")\n",
    "\n",
    "print(\"Filtered search results (beginner level):\")\n",
    "for doc in filtered_docs:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae1036",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Building a RAG System {#rag-system}\n",
    "\n",
    "### 8.1 Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c271d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Sample long document\n",
    "long_document = \"\"\"\n",
    "Vector embeddings have revolutionized natural language processing and information retrieval. \n",
    "These numerical representations capture semantic meaning in high-dimensional spaces, enabling \n",
    "machines to understand and process human language more effectively.\n",
    "\n",
    "The concept of embeddings dates back to word2vec, which introduced the idea of representing \n",
    "words as dense vectors. This approach moved beyond simple one-hot encodings to capture \n",
    "relationships between words based on their context and usage patterns.\n",
    "\n",
    "Modern embedding models like BERT, RoBERTa, and GPT have further advanced the field by \n",
    "providing contextualized representations. These models understand that the same word can \n",
    "have different meanings depending on its context, leading to more accurate and nuanced \n",
    "text understanding.\n",
    "\n",
    "Vector databases have emerged as specialized storage solutions for embeddings, offering \n",
    "efficient similarity search capabilities. Popular vector databases include Pinecone, \n",
    "Weaviate, Chroma, and FAISS, each with unique features and optimization strategies.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) represents a significant advancement in language \n",
    "model applications. By combining retrieval mechanisms with generative models, RAG systems \n",
    "can provide more accurate, up-to-date, and contextually relevant responses.\n",
    "\"\"\"\n",
    "\n",
    "# Split document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(long_document)\n",
    "print(f\"Document split into {len(chunks)} chunks\")\n",
    "\n",
    "# Create vector store from chunks\n",
    "rag_vectorstore = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=hf_embeddings,\n",
    "    persist_directory=\"./rag_chroma\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719292ea",
   "metadata": {},
   "source": [
    "### 8.2 RAG Chain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize language model\n",
    "llm = OpenAI(temperature=0.7, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Create retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=rag_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Test the RAG system\n",
    "questions = [\n",
    "    \"What are vector embeddings?\",\n",
    "    \"How have embedding models evolved over time?\",\n",
    "    \"What is Retrieval Augmented Generation?\",\n",
    "    \"What are some popular vector databases?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = qa_chain({\"query\": question})\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    \n",
    "    print(\"\\nSource documents:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"{i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdf5cb",
   "metadata": {},
   "source": [
    "### 8.3 Custom RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5106173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRAG:\n",
    "    def __init__(self, vectorstore, llm, top_k=3):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def retrieve_context(self, query):\n",
    "        \"\"\"Retrieve relevant documents for the query\"\"\"\n",
    "        docs = self.vectorstore.similarity_search(query, k=self.top_k)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        return context, docs\n",
    "    \n",
    "    def generate_response(self, query, context):\n",
    "        \"\"\"Generate response using retrieved context\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Context information:\n",
    "        {context}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Based on the context information provided, please answer the question. \n",
    "        If the context doesn't contain enough information to answer the question, \n",
    "        please say so.\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm(prompt)\n",
    "        return response\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Main query method\"\"\"\n",
    "        context, source_docs = self.retrieve_context(question)\n",
    "        response = self.generate_response(question, context)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': response,\n",
    "            'context': context,\n",
    "            'source_documents': source_docs\n",
    "        }\n",
    "\n",
    "# Initialize custom RAG\n",
    "custom_rag = CustomRAG(rag_vectorstore, llm)\n",
    "\n",
    "# Test custom RAG\n",
    "test_query = \"What makes modern embedding models better than older ones?\"\n",
    "result = custom_rag.query(test_query)\n",
    "\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\\nNumber of source documents: {len(result['source_documents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e95cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Advanced Techniques {#advanced-techniques}\n",
    "\n",
    "### 9.1 Hybrid Search (Semantic + Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# Create BM25 retriever for keyword search\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "# Create vector retriever for semantic search\n",
    "vector_retriever = rag_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Combine both retrievers\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.3, 0.7]  # Give more weight to semantic search\n",
    ")\n",
    "\n",
    "# Test hybrid search\n",
    "hybrid_query = \"vector database optimization\"\n",
    "hybrid_docs = ensemble_retriever.get_relevant_documents(hybrid_query)\n",
    "\n",
    "print(f\"Hybrid search results for: {hybrid_query}\")\n",
    "for i, doc in enumerate(hybrid_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00509c38",
   "metadata": {},
   "source": [
    "### 9.2 Embedding Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a278cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Sample training data for fine-tuning\n",
    "training_examples = [\n",
    "    InputExample(texts=['Vector embeddings', 'Numerical representations of text'], label=1.0),\n",
    "    InputExample(texts=['LangChain framework', 'Building LLM applications'], label=1.0),\n",
    "    InputExample(texts=['RAG system', 'Retrieval Augmented Generation'], label=1.0),\n",
    "    InputExample(texts=['Vector database', 'Machine learning models'], label=0.3),\n",
    "    InputExample(texts=['Semantic search', 'Keyword matching'], label=0.2),\n",
    "]\n",
    "\n",
    "# Fine-tune embedding model (example - requires more data in practice)\n",
    "def fine_tune_embeddings(model_name, training_examples, output_path):\n",
    "    # Load pre-trained model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Create data loader\n",
    "    train_dataloader = DataLoader(training_examples, shuffle=True, batch_size=16)\n",
    "    \n",
    "    # Define loss function\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    \n",
    "    # Fine-tune (minimal example)\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        warmup_steps=10,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Note: This is a simplified example - real fine-tuning requires more data and careful setup\n",
    "print(\"Fine-tuning example prepared (requires substantial training data for real use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be6624",
   "metadata": {},
   "source": [
    "### 9.3 Multi-modal Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of handling different content types\n",
    "class MultiModalEmbedder:\n",
    "    def __init__(self, text_embedder, image_embedder=None):\n",
    "        self.text_embedder = text_embedder\n",
    "        self.image_embedder = image_embedder\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        return self.text_embedder.embed_query(text)\n",
    "    \n",
    "    def embed_mixed_content(self, content_list):\n",
    "        embeddings = []\n",
    "        for content in content_list:\n",
    "            if content['type'] == 'text':\n",
    "                emb = self.embed_text(content['data'])\n",
    "                embeddings.append(emb)\n",
    "            # Add image handling when available\n",
    "        return embeddings\n",
    "\n",
    "# Initialize multi-modal embedder\n",
    "multi_modal = MultiModalEmbedder(hf_embeddings)\n",
    "\n",
    "# Example usage\n",
    "mixed_content = [\n",
    "    {'type': 'text', 'data': 'Vector embeddings in machine learning'},\n",
    "    {'type': 'text', 'data': 'Deep learning for natural language processing'}\n",
    "]\n",
    "\n",
    "embeddings = multi_modal.embed_mixed_content(mixed_content)\n",
    "print(f\"Generated {len(embeddings)} embeddings for mixed content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bc753",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Best Practices {#best-practices}\n",
    "\n",
    "### 10.1 Embedding Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embedding_model(model, test_queries, test_documents):\n",
    "    \"\"\"Evaluate embedding model performance\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        query_emb = model.embed_query(query)\n",
    "        doc_embs = model.embed_documents(test_documents)\n",
    "        \n",
    "        similarities = [\n",
    "            cosine_similarity([query_emb], [doc_emb])[0][0]\n",
    "            for doc_emb in doc_embs\n",
    "        ]\n",
    "        \n",
    "        # Get top-k most similar documents\n",
    "        top_indices = np.argsort(similarities)[::-1][:3]\n",
    "        results[query] = {\n",
    "            'similarities': similarities,\n",
    "            'top_docs': [test_documents[i] for i in top_indices],\n",
    "            'top_scores': [similarities[i] for i in top_indices]\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different models\n",
    "test_queries = [\n",
    "    \"machine learning algorithms\",\n",
    "    \"natural language processing\",\n",
    "    \"vector similarity search\"\n",
    "]\n",
    "\n",
    "test_docs = [\n",
    "    \"Machine learning uses algorithms to learn patterns from data\",\n",
    "    \"NLP helps computers understand human language\",\n",
    "    \"Vector search finds similar items in high-dimensional spaces\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Transformers revolutionized natural language understanding\"\n",
    "]\n",
    "\n",
    "# Evaluate models\n",
    "openai_results = evaluate_embedding_model(openai_embeddings, test_queries, test_docs)\n",
    "hf_results = evaluate_embedding_model(hf_embeddings, test_queries, test_docs)\n",
    "\n",
    "print(\"Model evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52028f",
   "metadata": {},
   "source": [
    "### 10.2 Optimization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking strategies\n",
    "def optimize_chunking(text, chunk_sizes, overlap_ratios):\n",
    "    \"\"\"Test different chunking strategies\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        for overlap_ratio in overlap_ratios:\n",
    "            overlap = int(chunk_size * overlap_ratio)\n",
    "            \n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=overlap\n",
    "            )\n",
    "            \n",
    "            chunks = splitter.split_text(text)\n",
    "            \n",
    "            results[f\"size_{chunk_size}_overlap_{overlap_ratio}\"] = {\n",
    "                'num_chunks': len(chunks),\n",
    "                'avg_chunk_length': np.mean([len(chunk) for chunk in chunks]),\n",
    "                'chunks': chunks\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test chunking strategies\n",
    "chunk_sizes = [100, 200, 500]\n",
    "overlap_ratios = [0.1, 0.2, 0.3]\n",
    "\n",
    "chunking_results = optimize_chunking(long_document, chunk_sizes, overlap_ratios)\n",
    "\n",
    "print(\"Chunking optimization results:\")\n",
    "for strategy, result in chunking_results.items():\n",
    "    print(f\"{strategy}: {result['num_chunks']} chunks, avg length: {result['avg_chunk_length']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36dbde6",
   "metadata": {},
   "source": [
    "### 10.3 Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def monitor_performance(func):\n",
    "    \"\"\"Decorator to monitor function performance\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"{func.__name__} executed in {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@monitor_performance\n",
    "def batch_embed_documents(embedder, documents, batch_size=10):\n",
    "    \"\"\"Embed documents in batches for better performance\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        batch_embeddings = embedder.embed_documents(batch)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Test batch processing\n",
    "large_doc_set = sample_texts * 10  # Create larger dataset\n",
    "batch_embeddings = batch_embed_documents(hf_embeddings, large_doc_set)\n",
    "print(f\"Processed {len(batch_embeddings)} embeddings in batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8e16c",
   "metadata": {},
   "source": [
    "### 10.4 Quality Assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_embeddings(embeddings, expected_dim=None, check_norms=True):\n",
    "    \"\"\"Validate embedding quality\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check dimensions\n",
    "    if expected_dim and len(embeddings[0]) != expected_dim:\n",
    "        issues.append(f\"Dimension mismatch: expected {expected_dim}, got {len(embeddings[0])}\")\n",
    "    \n",
    "    # Check for NaN or infinite values\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        if np.any(np.isnan(emb)):\n",
    "            issues.append(f\"NaN values found in embedding {i}\")\n",
    "        if np.any(np.isinf(emb)):\n",
    "            issues.append(f\"Infinite values found in embedding {i}\")\n",
    "    \n",
    "    # Check norms (optional)\n",
    "    if check_norms:\n",
    "        norms = [np.linalg.norm(emb) for emb in embeddings]\n",
    "        if any(norm < 0.1 or norm > 10 for norm in norms):\n",
    "            issues.append(\"Unusual embedding norms detected\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Validate embeddings\n",
    "validation_issues = validate_embeddings(doc_embeddings, expected_dim=1536)\n",
    "if validation_issues:\n",
    "    for issue in validation_issues:\n",
    "        print(f\"Warning: {issue}\")\n",
    "else:\n",
    "    print(\"All embeddings passed validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002df71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook covered vector embeddings from basic concepts to advanced implementations using Python and LangChain. Key takeaways include:\n",
    "\n",
    "1. **Understanding vector spaces** and semantic similarity\n",
    "2. **Choosing appropriate embedding models** for your use case\n",
    "3. **Building efficient vector stores** for similarity search\n",
    "4. **Implementing RAG systems** for enhanced language model applications\n",
    "5. **Optimizing performance** through proper chunking and batching\n",
    "6. **Monitoring and validating** embedding quality\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different embedding models for your specific domain\n",
    "- Implement hybrid search strategies combining semantic and keyword search\n",
    "- Explore fine-tuning embeddings for specialized applications\n",
    "- Build production-ready RAG systems with proper error handling and monitoring\n",
    "\n",
    "### Resources:\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Vector Database Comparison](https://github.com/vector-databases/vector-databases)\n",
    "- [Embedding Model Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
