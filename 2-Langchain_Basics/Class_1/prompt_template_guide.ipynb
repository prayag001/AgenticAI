{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate vs ChatPromptTemplate - Complete Guide\n",
    "\n",
    "#### Quick Summary\n",
    "<div style=\"font-size:16px\">\n",
    "\n",
    "| Aspect | PromptTemplate | ChatPromptTemplate |\n",
    "|--------|----------------|-------------------|\n",
    "| **Use Case** | Simple text completion | Chat conversations |\n",
    "| **Modern LLMs** | ‚ùå Limited | ‚úÖ Optimal |\n",
    "| **Pydantic** | ‚ö†Ô∏è Works but limited | ‚úÖ Perfect |\n",
    "| **System Instructions** | ‚ùå No native support | ‚úÖ Built-in |\n",
    "| **Recommendation** | Legacy only | **Always prefer** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px\">\n",
    "\n",
    "#### üìã `PromptTemplate` Methods & Properties\n",
    "\n",
    "| Method / Property           | Description                    | Example                                               |\n",
    "|----------------------------|--------------------------------|-------------------------------------------------------|\n",
    "| `from_template(template)`  | Create from string template     | `PromptTemplate.from_template(\"Tell me about {topic}\")` |\n",
    "| `from_file(template_file)` | Load from file                  | `PromptTemplate.from_file(\"prompt.txt\")`             |\n",
    "| `format(**kwargs)`         | Format with variables           | `prompt.format(topic=\"AI\")`                          |\n",
    "| `format_prompt(**kwargs)`  | Returns `PromptValue` object    | `prompt.format_prompt(topic=\"AI\")`                   |\n",
    "| `invoke(input_dict)`       | Execute with input              | `prompt.invoke({\"topic\": \"AI\"})`                     |\n",
    "| `partial(**kwargs)`        | Partially fill variables        | `prompt.partial(style=\"professional\")`               |\n",
    "| `input_variables`          | List of variables               | `prompt.input_variables`                             |\n",
    "| `template`                 | Template string                 | `prompt.template`                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px\">\n",
    "\n",
    "#### üí¨ `ChatPromptTemplate` Methods & Properties\n",
    "\n",
    "| Method / Property             | Description                     | Example                                                                                   |\n",
    "|------------------------------|----------------------------------|-------------------------------------------------------------------------------------------|\n",
    "| `from_messages(messages)`    | Create from message list         | `ChatPromptTemplate.from_messages([(\"system\", \"You are helpful\"), (\"human\", \"{query}\")])` |\n",
    "| `from_template(template)`    | Create single human message      | `ChatPromptTemplate.from_template(\"Answer: {question}\")`                                 |\n",
    "| `format_messages(**kwargs)`  | Format to message list           | `prompt.format_messages(query=\"Hello\")`                                                  |\n",
    "| `format_prompt(**kwargs)`    | Returns `ChatPromptValue`        | `prompt.format_prompt(query=\"Hello\")`                                                    |\n",
    "| `invoke(input_dict)`         | Execute with input               | `prompt.invoke({\"query\": \"Hello\"})`                                                      |\n",
    "| `partial(**kwargs)`          | Partially fill variables         | `prompt.partial(context=\"AI Assistant\")`                                                 |\n",
    "| `append(message)`            | Add message to template          | `prompt.append((\"human\", \"Additional: {extra}\"))`                                        |\n",
    "| `extend(messages)`           | Add multiple messages            | `prompt.extend([(\"system\", \"Rule: {rule}\")])`                                            |\n",
    "| `input_variables`            | List of variables                | `prompt.input_variables`                                                                 |\n",
    "| `messages`                   | List of message templates        | `prompt.messages`                                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:16px\">\n",
    "\n",
    "#### üí¨ `Message Types in ChatPromptTemplate` \n",
    "\n",
    "| Message Type  | Usage                 | Example                                                 |\n",
    "| ------------- | --------------------- | ------------------------------------------------------- |\n",
    "| `\"system\"`    | System instructions   | `(\"system\", \"You are a helpful assistant\")`             |\n",
    "| `\"human\"`     | Human/user input      | `(\"human\", \"What is {topic}?\")`                         |\n",
    "| `\"ai\"`        | AI/assistant response | `(\"ai\", \"I understand you want to know about {topic}\")` |\n",
    "| `\"assistant\"` | Same as `\"ai\"`        | `(\"assistant\", \"Let me help with {task}\")`              |\n",
    "| `\"user\"`      | Same as `\"human\"`     | `(\"user\", \"Please explain {concept}\")`                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq  # or your preferred LLM\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PromptTemplate - Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PROMPTTEMPLATE EXAMPLES =====\n",
    "\n",
    "# Method 1: from_template (most common)\n",
    "prompt1 = PromptTemplate.from_template(\n",
    "    \"Write a {style} email about {topic} to {recipient}\"\n",
    ")\n",
    "\n",
    "# Method 2: Constructor with explicit variables\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Explain {concept} in {difficulty} terms\",\n",
    "    input_variables=[\"concept\", \"difficulty\"]\n",
    ")\n",
    "\n",
    "# Method 3: With partial variables\n",
    "prompt3 = PromptTemplate(\n",
    "    template=\"Write a {style} summary of {content}\",\n",
    "    input_variables=[\"content\"],\n",
    "    partial_variables={\"style\": \"professional\"}\n",
    ")\n",
    "\n",
    "print(\"PromptTemplate Examples:\")\n",
    "print(f\"Variables in prompt1: {prompt1.input_variables}\")\n",
    "print(f\"Template: {prompt1.template}\")\n",
    "print(f\"\\nFormatted: {prompt1.format(style='formal', topic='AI', recipient='team')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChatPromptTemplate - Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHATPROMPTTEMPLATE EXAMPLES =====\n",
    "\n",
    "# Method 1: from_messages (most common)\n",
    "chat_prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful {role} assistant\"),\n",
    "    (\"human\", \"Help me with {task}\"),\n",
    "    (\"ai\", \"I'll help you with {task}. What specifically do you need?\"),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "# Method 2: from_template (creates single human message)\n",
    "chat_prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"Answer this question: {question}\"\n",
    ")\n",
    "\n",
    "# Method 3: With partial variables\n",
    "chat_prompt3 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role} specializing in {domain}\"),\n",
    "    (\"human\", \"Question: {query}\")\n",
    "], partial_variables={\"role\": \"expert\", \"domain\": \"Python programming\"})\n",
    "\n",
    "print(\"ChatPromptTemplate Examples:\")\n",
    "print(f\"Variables in chat_prompt1: {chat_prompt1.input_variables}\")\n",
    "print(f\"Number of messages: {len(chat_prompt1.messages)}\")\n",
    "print(f\"\\nFormatted messages:\")\n",
    "formatted = chat_prompt1.format_messages(role=\"coding\", task=\"debugging\", query=\"Fix this error\")\n",
    "for msg in formatted:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Message Types in ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message Types Demo:\n",
      "1. SYSTEM: You are a friendly assistant\n",
      "2. HUMAN: Hi, I need help with Python\n",
      "3. AI: Hello! I'd be happy to help with Python\n",
      "4. HUMAN: Can you explain functions?\n",
      "5. AI: Sure! functions is...\n",
      "6. HUMAN: Show me an example\n"
     ]
    }
   ],
   "source": [
    "# ===== MESSAGE TYPES =====\n",
    "\n",
    "message_types_example = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {personality} assistant\"),      # System instructions\n",
    "    (\"human\", \"Hi, I need help with {topic}\"),           # Human/User input\n",
    "    (\"ai\", \"Hello! I'd be happy to help with {topic}\"),  # AI/Assistant response\n",
    "    (\"user\", \"Can you explain {concept}?\"),              # Alternative to \"human\"\n",
    "    (\"assistant\", \"Sure! {concept} is...\"),              # Alternative to \"ai\"\n",
    "    (\"human\", \"{final_question}\")\n",
    "])\n",
    "\n",
    "print(\"Message Types Demo:\")\n",
    "formatted = message_types_example.format_messages(\n",
    "    personality=\"friendly\",\n",
    "    topic=\"Python\",\n",
    "    concept=\"functions\",\n",
    "    final_question=\"Show me an example\"\n",
    ")\n",
    "\n",
    "for i, msg in enumerate(formatted):\n",
    "    print(f\"{i+1}. {msg.type.upper()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Common Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== METHODS COMPARISON =====\n",
    "\n",
    "# Create examples\n",
    "simple_prompt = PromptTemplate.from_template(\"Explain {topic} in {style} way\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert teacher\"),\n",
    "    (\"human\", \"Explain {topic} in {style} way\")\n",
    "])\n",
    "\n",
    "print(\"=== COMMON METHODS ===\")\n",
    "\n",
    "# 1. Input variables\n",
    "print(f\"PromptTemplate variables: {simple_prompt.input_variables}\")\n",
    "print(f\"ChatPromptTemplate variables: {chat_prompt.input_variables}\")\n",
    "\n",
    "# 2. Format methods\n",
    "print(\"\\n=== FORMAT METHODS ===\")\n",
    "print(\"PromptTemplate.format():\")\n",
    "print(simple_prompt.format(topic=\"AI\", style=\"simple\"))\n",
    "\n",
    "print(\"\\nChatPromptTemplate.format_messages():\")\n",
    "chat_messages = chat_prompt.format_messages(topic=\"AI\", style=\"simple\")\n",
    "for msg in chat_messages:\n",
    "    print(f\"  {msg.type}: {msg.content}\")\n",
    "\n",
    "# 3. Invoke method\n",
    "print(\"\\n=== INVOKE METHODS ===\")\n",
    "prompt_value = simple_prompt.invoke({\"topic\": \"Python\", \"style\": \"beginner-friendly\"})\n",
    "chat_value = chat_prompt.invoke({\"topic\": \"Python\", \"style\": \"beginner-friendly\"})\n",
    "\n",
    "print(f\"PromptTemplate.invoke() returns: {type(prompt_value)}\")\n",
    "print(f\"ChatPromptTemplate.invoke() returns: {type(chat_value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pydantic Integration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PYDANTIC INTEGRATION =====\n",
    "\n",
    "# Define Pydantic model\n",
    "class Product(BaseModel):\n",
    "    name: str = Field(description=\"Product name\")\n",
    "    description: str = Field(description=\"Product description\")\n",
    "    price: float = Field(description=\"Price in USD\")\n",
    "    category: str = Field(description=\"Product category\")\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=Product)\n",
    "\n",
    "# OPTION 1: PromptTemplate (works but limited)\n",
    "prompt_template_pydantic = PromptTemplate.from_template(\n",
    "    \"Extract product information for: {query}\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "# OPTION 2: ChatPromptTemplate (RECOMMENDED)\n",
    "chat_prompt_pydantic = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product information specialist. Extract structured product data.\"),\n",
    "    (\"human\", \"Product: {query}\\n\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# OPTION 3: ChatPromptTemplate with partial_variables (BEST)\n",
    "chat_prompt_partial = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product specialist. Return structured product data.\"),\n",
    "    (\"human\", \"Extract information for: {query}\\n\\n{format_instructions}\")\n",
    "], partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "print(\"=== PYDANTIC EXAMPLES ===\")\n",
    "print(\"Format instructions preview:\")\n",
    "print(parser.get_format_instructions()[:200] + \"...\")\n",
    "\n",
    "print(\"\\nOption 3 (Best) - Only need to provide query:\")\n",
    "formatted = chat_prompt_partial.format_messages(query=\"iPhone 16 Pro Max\")\n",
    "print(f\"Human message: {formatted[1].content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Chain Examples with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMPLETE CHAIN EXAMPLES =====\n",
    "\n",
    "# Setup (uncomment and configure as needed)\n",
    "# load_dotenv()\n",
    "# llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "# Example chains (ready to use when LLM is configured)\n",
    "\n",
    "# CHAIN 1: Simple PromptTemplate chain\n",
    "simple_chain_template = \"\"\"\n",
    "simple_prompt = PromptTemplate.from_template(\n",
    "    \"Write a {tone} explanation of {topic} in {length} words\"\n",
    ")\n",
    "simple_chain = simple_prompt | llm\n",
    "result = simple_chain.invoke({\n",
    "    \"tone\": \"friendly\",\n",
    "    \"topic\": \"machine learning\",\n",
    "    \"length\": \"100\"\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "# CHAIN 2: ChatPromptTemplate chain\n",
    "chat_chain_template = \"\"\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role} assistant\"),\n",
    "    (\"human\", \"Explain {topic} in {style} terms\")\n",
    "])\n",
    "chat_chain = chat_prompt | llm\n",
    "result = chat_chain.invoke({\n",
    "    \"role\": \"helpful\",\n",
    "    \"topic\": \"neural networks\",\n",
    "    \"style\": \"simple\"\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "# CHAIN 3: Pydantic chain (RECOMMENDED)\n",
    "pydantic_chain_template = \"\"\"\n",
    "parser = PydanticOutputParser(pydantic_object=Product)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product specialist. Return structured data.\"),\n",
    "    (\"human\", \"Product info for: {query}\\n\\n{format_instructions}\")\n",
    "], partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "result = chain.invoke({\"query\": \"MacBook Pro M3\"})\n",
    "# Returns: Product(name=\"MacBook Pro M3\", description=\"...\", price=2499.0, category=\"Laptop\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== CHAIN TEMPLATES ===\")\n",
    "print(\"1. Simple PromptTemplate Chain:\")\n",
    "print(simple_chain_template)\n",
    "print(\"\\n2. ChatPromptTemplate Chain:\")\n",
    "print(chat_chain_template)\n",
    "print(\"\\n3. Pydantic Chain (RECOMMENDED):\")\n",
    "print(pydantic_chain_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When to Use Which - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DECISION TREE =====\n",
    "\n",
    "decision_guide = \"\"\"\n",
    "ü§î WHICH TEMPLATE TO USE?\n",
    "\n",
    "‚îå‚îÄ Are you using modern chat models (GPT-4, Claude, etc.)?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí Use ChatPromptTemplate ‚úÖ\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Are you using legacy completion models?\n",
    "‚îÇ      ‚îú‚îÄ YES ‚Üí Use PromptTemplate\n",
    "‚îÇ      ‚îî‚îÄ NO ‚Üí Use ChatPromptTemplate anyway (future-proof)\n",
    "‚îÇ\n",
    "‚îå‚îÄ Do you need system instructions?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí ChatPromptTemplate ‚úÖ\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Either works, but ChatPromptTemplate preferred\n",
    "‚îÇ\n",
    "‚îå‚îÄ Are you using Pydantic output parsing?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí ChatPromptTemplate with partial_variables ‚úÖ\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Either works\n",
    "‚îÇ\n",
    "‚îå‚îÄ Is this a conversation/multi-turn interaction?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí ChatPromptTemplate ‚úÖ\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Either works\n",
    "‚îÇ\n",
    "‚îå‚îÄ Do you want the most modern, future-proof approach?\n",
    "‚îÇ  ‚îú‚îÄ YES ‚Üí ChatPromptTemplate ‚úÖ\n",
    "‚îÇ  ‚îî‚îÄ NO ‚Üí Why not? ü§∑‚Äç‚ôÇÔ∏è\n",
    "\n",
    "üèÜ WINNER: ChatPromptTemplate in 90% of cases!\n",
    "\"\"\"\n",
    "\n",
    "print(decision_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices & Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BEST PRACTICES =====\n",
    "\n",
    "best_practices = \"\"\"\n",
    "‚úÖ DO's:\n",
    "1. Use ChatPromptTemplate for modern applications\n",
    "2. Use partial_variables for static values (like format_instructions)\n",
    "3. Keep system messages clear and specific\n",
    "4. Use descriptive variable names in templates\n",
    "5. Test your prompts with different inputs\n",
    "\n",
    "‚ùå DON'Ts:\n",
    "1. Don't use PromptTemplate for chat models unless necessary\n",
    "2. Don't hardcode values that could be variables\n",
    "3. Don't make system messages too long\n",
    "4. Don't forget to validate your Pydantic models\n",
    "5. Don't mix message types unnecessarily\n",
    "\n",
    "üéØ PRODUCTION READY TEMPLATE:\n",
    "\"\"\"\n",
    "\n",
    "# Production-ready template example\n",
    "production_example = \"\"\"\n",
    "# Define your Pydantic model\n",
    "class ProductInfo(BaseModel):\n",
    "    name: str = Field(description=\"Full product name\")\n",
    "    description: str = Field(description=\"Detailed description\")\n",
    "    price: float = Field(description=\"Price in USD\")\n",
    "    availability: str = Field(description=\"In stock, Out of stock, Limited\")\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=ProductInfo)\n",
    "\n",
    "# Create prompt with partial variables\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an expert product analyst. Extract accurate product information. \"\n",
    "     \"If information is not available, use 'Unknown' for strings and 0.0 for prices.\"),\n",
    "    (\"human\", \"Analyze this product: {product_query}\\n\\n{format_instructions}\")\n",
    "], partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Use it\n",
    "try:\n",
    "    result = chain.invoke({\"product_query\": \"iPhone 16 Pro Max 256GB\"})\n",
    "    print(f\"Product: {result.name}\")\n",
    "    print(f\"Price: ${result.price}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PRODUCTION EXAMPLE:\")\n",
    "print(production_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Reference Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== QUICK REFERENCE =====\n",
    "\n",
    "cheat_sheet = \"\"\"\n",
    "üìã QUICK REFERENCE CHEAT SHEET\n",
    "\n",
    "üîπ BASIC CREATION:\n",
    "# PromptTemplate\n",
    "PromptTemplate.from_template(\"Text with {variable}\")\n",
    "\n",
    "# ChatPromptTemplate  \n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are {role}\"),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "üîπ WITH PARTIAL VARIABLES:\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are {role}\"),\n",
    "    (\"human\", \"{query}\")\n",
    "], partial_variables={\"role\": \"helpful assistant\"})\n",
    "\n",
    "üîπ PYDANTIC INTEGRATION:\n",
    "parser = PydanticOutputParser(pydantic_object=YourModel)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Return structured data\"),\n",
    "    (\"human\", \"{query}\\n\\n{format_instructions}\")\n",
    "], partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "üîπ COMMON CHAINS:\n",
    "# Simple\n",
    "chain = prompt | llm\n",
    "\n",
    "# With parsing\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Usage\n",
    "result = chain.invoke({\"query\": \"your input\"})\n",
    "\n",
    "üîπ MESSAGE TYPES:\n",
    "\"system\"    - System instructions\n",
    "\"human\"     - User input\n",
    "\"ai\"        - AI response\n",
    "\"user\"      - Same as human\n",
    "\"assistant\" - Same as ai\n",
    "\"\"\"\n",
    "\n",
    "print(cheat_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Recommendation\n",
    "\n",
    "**For 90% of use cases, use ChatPromptTemplate with partial_variables:**\n",
    "\n",
    "```python\n",
    "# This is your go-to pattern\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "], partial_variables={\"static_var\": \"static_value\"})\n",
    "\n",
    "chain = prompt | llm | parser  # Add parser if needed\n",
    "result = chain.invoke({\"user_input\": \"Hello\"})\n",
    "```\n",
    "\n",
    "**Only use PromptTemplate for:**\n",
    "- Legacy completion models\n",
    "- Very simple single-shot text generation\n",
    "- Backward compatibility\n",
    "\n",
    "---\n",
    "*Save this notebook as your reference guide! üìñ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Scenario**               | **Use `PromptTemplate`** | **Use `ChatPromptTemplate`** |\n",
    "| -------------------------- | ------------------------ | ---------------------------- |\n",
    "| Simple text completion     | ‚úÖ Perfect                | ‚ùå Overkill                   |\n",
    "| Chat conversations         | ‚ùå Won't work well        | ‚úÖ Perfect                    |\n",
    "| System instructions needed | ‚ùå No native support      | ‚úÖ Built-in support           |\n",
    "| Multi-turn conversations   | ‚ùå Complex to manage      | ‚úÖ Natural fit                |\n",
    "| OpenAI GPT models          | ‚ö†Ô∏è Works but limited     | ‚úÖ Optimal                    |\n",
    "| Anthropic Claude           | ‚ö†Ô∏è Works but limited     | ‚úÖ Optimal                    |\n",
    "| Legacy completion models   | ‚úÖ Perfect                | ‚ö†Ô∏è Gets converted            |\n",
    "| Complex role-playing       | ‚ùå Difficult              | ‚úÖ Easy                       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
