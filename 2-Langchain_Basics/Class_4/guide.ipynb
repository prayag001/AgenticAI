{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector Search at Scale: Cloud vs Local DBs - Build a RAG App with Pinecone, LangChain & OpenAI\n",
        "\n",
        "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) application using vector search technologies, comparing cloud-based solutions like Pinecone with local alternatives.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to Vector Search](#introduction)\n",
        "2. [Cloud vs Local Vector Databases](#comparison)\n",
        "3. [Setting Up the Environment](#setup)\n",
        "4. [Building with Pinecone (Cloud)](#pinecone)\n",
        "5. [Building with Local Vector DB](#local)\n",
        "6. [RAG Application Implementation](#rag)\n",
        "7. [Performance Comparison](#performance)\n",
        "8. [Conclusion](#conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Vector Search {#introduction}\n",
        "\n",
        "Vector search is a fundamental component of modern AI applications, enabling semantic similarity search across large datasets. Unlike traditional keyword-based search, vector search uses high-dimensional embeddings to find semantically similar content.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Embeddings**: Dense vector representations of text, images, or other data\n",
        "- **Similarity Metrics**: Methods to measure similarity between vectors (cosine, euclidean, dot product)\n",
        "- **Vector Databases**: Specialized databases optimized for storing and querying high-dimensional vectors\n",
        "- **RAG**: Retrieval-Augmented Generation combines retrieval of relevant information with generative AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cloud vs Local Vector Databases {#comparison}\n",
        "\n",
        "| Aspect | Cloud (Pinecone) | Local (FAISS/Chroma) |\n",
        "|--------|------------------|----------------------|\n",
        "| **Scalability** | Highly scalable, managed | Limited by hardware |\n",
        "| **Cost** | Pay-per-use, subscription | Hardware + maintenance |\n",
        "| **Latency** | Network dependent | Very low latency |\n",
        "| **Setup Complexity** | Minimal setup | Requires configuration |\n",
        "| **Data Privacy** | Data leaves premises | Complete data control |\n",
        "| **Maintenance** | Fully managed | Self-maintained |\n",
        "| **Performance** | Optimized for scale | Optimized for local use |\n",
        "\n",
        "### When to Choose Cloud:\n",
        "- Large-scale applications\n",
        "- Variable workloads\n",
        "- Limited infrastructure resources\n",
        "- Need for high availability\n",
        "\n",
        "### When to Choose Local:\n",
        "- Data privacy requirements\n",
        "- Low-latency needs\n",
        "- Cost optimization for consistent workloads\n",
        "- Full control over infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setting Up the Environment {#setup}\n",
        "\n",
        "First, let's install the required packages for both cloud and local implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pinecone-client langchain openai chromadb faiss-cpu sentence-transformers python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone, Chroma, FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "import time\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration and API Keys\n",
        "\n",
        "Set up your API keys and configuration. Create a `.env` file with your credentials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
        "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')  # e.g., 'us-west1-gcp'\n",
        "\n",
        "# Verify API keys are loaded\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"⚠️ OPENAI_API_KEY not found. Please set it in your .env file.\")\n",
        "if not PINECONE_API_KEY:\n",
        "    print(\"⚠️ PINECONE_API_KEY not found. Please set it in your .env file.\")\n",
        "else:\n",
        "    print(\"✅ API keys loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sample Data Preparation\n",
        "\n",
        "Let's create some sample documents to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents for testing\n",
        "sample_documents = [\n",
        "    \"Vector databases are specialized databases designed to store and query high-dimensional vectors efficiently. They use advanced indexing techniques like HNSW, IVF, and LSH to enable fast similarity search.\",\n",
        "    \"Pinecone is a cloud-native vector database that provides managed vector search capabilities. It offers features like real-time updates, filtering, and horizontal scaling.\",\n",
        "    \"FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search and clustering of dense vectors. It's optimized for both CPU and GPU execution.\",\n",
        "    \"ChromaDB is an open-source embedding database that focuses on simplicity and developer experience. It supports multiple embedding models and provides easy-to-use APIs.\",\n",
        "    \"Retrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval. It helps reduce hallucinations and provides more accurate, up-to-date responses.\",\n",
        "    \"Embeddings are dense vector representations of data that capture semantic meaning. OpenAI's text-embedding-ada-002 model produces 1536-dimensional embeddings for text.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. It provides abstractions for vector stores, document loaders, and retrieval chains.\",\n",
        "    \"Semantic search goes beyond keyword matching to understand the intent and contextual meaning of search queries. It uses embeddings to find semantically similar content.\"\n",
        "]\n",
        "\n",
        "print(f\"Prepared {len(sample_documents)} sample documents for testing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Building with Pinecone (Cloud Solution) {#pinecone}\n",
        "\n",
        "Let's implement a vector search solution using Pinecone as our cloud-based vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Pinecone\n",
        "def setup_pinecone():\n",
        "    try:\n",
        "        pinecone.init(\n",
        "            api_key=PINECONE_API_KEY,\n",
        "            environment=PINECONE_ENVIRONMENT\n",
        "        )\n",
        "        print(\"✅ Pinecone initialized successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing Pinecone: {e}\")\n",
        "        return False\n",
        "\n",
        "pinecone_ready = setup_pinecone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or connect to Pinecone index\n",
        "INDEX_NAME = \"rag-demo-index\"\n",
        "DIMENSION = 1536  # OpenAI ada-002 embedding dimension\n",
        "\n",
        "def create_pinecone_index():\n",
        "    if not pinecone_ready:\n",
        "        print(\"❌ Pinecone not ready\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Check if index exists\n",
        "        if INDEX_NAME not in pinecone.list_indexes():\n",
        "            # Create index\n",
        "            pinecone.create_index(\n",
        "                name=INDEX_NAME,\n",
        "                dimension=DIMENSION,\n",
        "                metric=\"cosine\"\n",
        "            )\n",
        "            print(f\"✅ Created new Pinecone index: {INDEX_NAME}\")\n",
        "        else:\n",
        "            print(f\"✅ Using existing Pinecone index: {INDEX_NAME}\")\n",
        "        \n",
        "        # Connect to index\n",
        "        index = pinecone.Index(INDEX_NAME)\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error with Pinecone index: {e}\")\n",
        "        return None\n",
        "\n",
        "pinecone_index = create_pinecone_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize embeddings and create Pinecone vector store\n",
        "def create_pinecone_vectorstore():\n",
        "    if not pinecone_index:\n",
        "        print(\"❌ Pinecone index not available\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Initialize OpenAI embeddings\n",
        "        embeddings = OpenAIEmbeddings(\n",
        "            openai_api_key=OPENAI_API_KEY,\n",
        "            model=\"text-embedding-ada-002\"\n",
        "        )\n",
        "        \n",
        "        # Create vector store from documents\n",
        "        vectorstore = Pinecone.from_texts(\n",
        "            texts=sample_documents,\n",
        "            embedding=embeddings,\n",
        "            index_name=INDEX_NAME\n",
        "        )\n",
        "        \n",
        "        print(f\"✅ Created Pinecone vector store with {len(sample_documents)} documents\")\n",
        "        return vectorstore, embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating Pinecone vector store: {e}\")\n",
        "        return None, None\n",
        "\n",
        "pinecone_vectorstore, openai_embeddings = create_pinecone_vectorstore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Building with Local Vector Database {#local}\n",
        "\n",
        "Now let's implement the same functionality using local vector databases (FAISS and ChromaDB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create FAISS vector store (local)\n",
        "def create_faiss_vectorstore():\n",
        "    try:\n",
        "        if not openai_embeddings:\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                openai_api_key=OPENAI_API_KEY,\n",
        "                model=\"text-embedding-ada-002\"\n",
        "            )\n",
        "        else:\n",
        "            embeddings = openai_embeddings\n",
        "        \n",
        "        # Create FAISS vector store\n",
        "        faiss_vectorstore = FAISS.from_texts(\n",
        "            texts=sample_documents,\n",
        "            embedding=embeddings\n",
        "        )\n",
        "        \n",
        "        # Save the vector store locally\n",
        "        faiss_vectorstore.save_local(\"faiss_index\")\n",
        "        \n",
        "        print(f\"✅ Created FAISS vector store with {len(sample_documents)} documents\")\n",
        "        return faiss_vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating FAISS vector store: {e}\")\n",
        "        return None\n",
        "\n",
        "faiss_vectorstore = create_faiss_vectorstore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ChromaDB vector store (local)\n",
        "def create_chroma_vectorstore():\n",
        "    try:\n",
        "        if not openai_embeddings:\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                openai_api_key=OPENAI_API_KEY,\n",
        "                model=\"text-embedding-ada-002\"\n",
        "            )\n",
        "        else:\n",
        "            embeddings = openai_embeddings\n",
        "        \n",
        "        # Create ChromaDB vector store\n",
        "        chroma_vectorstore = Chroma.from_texts(\n",
        "            texts=sample_documents,\n",
        "            embedding=embeddings,\n",
        "            persist_directory=\"./chroma_db\"\n",
        "        )\n",
        "        \n",
        "        # Persist the database\n",
        "        chroma_vectorstore.persist()\n",
        "        \n",
        "        print(f\"✅ Created ChromaDB vector store with {len(sample_documents)} documents\")\n",
        "        return chroma_vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating ChromaDB vector store: {e}\")\n",
        "        return None\n",
        "\n",
        "chroma_vectorstore = create_chroma_vectorstore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. RAG Application Implementation {#rag}\n",
        "\n",
        "Now let's build RAG applications using each vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize OpenAI LLM\n",
        "llm = OpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0.1,\n",
        "    model_name=\"gpt-3.5-turbo-instruct\"\n",
        ")\n",
        "\n",
        "print(\"✅ OpenAI LLM initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create RAG chains for each vector database\n",
        "def create_rag_chains():\n",
        "    chains = {}\n",
        "    \n",
        "    # Pinecone RAG chain\n",
        "    if pinecone_vectorstore:\n",
        "        chains['pinecone'] = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=pinecone_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        print(\"✅ Pinecone RAG chain created\")\n",
        "    \n",
        "    # FAISS RAG chain\n",
        "    if faiss_vectorstore:\n",
        "        chains['faiss'] = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=faiss_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        print(\"✅ FAISS RAG chain created\")\n",
        "    \n",
        "    # ChromaDB RAG chain\n",
        "    if chroma_vectorstore:\n",
        "        chains['chroma'] = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=chroma_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        print(\"✅ ChromaDB RAG chain created\")\n",
        "    \n",
        "    return chains\n",
        "\n",
        "rag_chains = create_rag_chains()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Performance Comparison {#performance}\n",
        "\n",
        "Let's test and compare the performance of different vector databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test queries\n",
        "test_queries = [\n",
        "    \"What is a vector database?\",\n",
        "    \"How does RAG work?\",\n",
        "    \"Compare Pinecone and FAISS\",\n",
        "    \"What are embeddings used for?\"\n",
        "]\n",
        "\n",
        "def test_rag_performance(chains, queries):\n",
        "    results = {}\n",
        "    \n",
        "    for chain_name, chain in chains.items():\n",
        "        print(f\"\\n🔍 Testing {chain_name.upper()} RAG Chain\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        chain_results = []\n",
        "        \n",
        "        for i, query in enumerate(queries, 1):\n",
        "            print(f\"\\nQuery {i}: {query}\")\n",
        "            \n",
        "            try:\n",
        "                # Measure response time\n",
        "                start_time = time.time()\n",
        "                response = chain({\"query\": query})\n",
        "                end_time = time.time()\n",
        "                \n",
        "                response_time = end_time - start_time\n",
        "                \n",
        "                print(f\"Response Time: {response_time:.2f} seconds\")\n",
        "                print(f\"Answer: {response['result'][:200]}...\")\n",
        "                print(f\"Sources: {len(response['source_documents'])} documents retrieved\")\n",
        "                \n",
        "                chain_results.append({\n",
        "                    'query': query,\n",
        "                    'response_time': response_time,\n",
        "                    'answer': response['result'],\n",
        "                    'sources_count': len(response['source_documents'])\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "                chain_results.append({\n",
        "                    'query': query,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "        \n",
        "        results[chain_name] = chain_results\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run performance tests\n",
        "performance_results = test_rag_performance(rag_chains, test_queries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze and display performance metrics\n",
        "def analyze_performance(results):\n",
        "    print(\"\\n📊 PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for chain_name, chain_results in results.items():\n",
        "        # Calculate average response time\n",
        "        response_times = [r['response_time'] for r in chain_results if 'response_time' in r]\n",
        "        \n",
        "        if response_times:\n",
        "            avg_time = np.mean(response_times)\n",
        "            min_time = np.min(response_times)\n",
        "            max_time = np.max(response_times)\n",
        "            \n",
        "            print(f\"\\n{chain_name.upper()} Performance:\")\n",
        "            print(f\"  Average Response Time: {avg_time:.3f}s\")\n",
        "            print(f\"  Min Response Time: {min_time:.3f}s\")\n",
        "            print(f\"  Max Response Time: {max_time:.3f}s\")\n",
        "            print(f\"  Successful Queries: {len(response_times)}/{len(chain_results)}\")\n",
        "        else:\n",
        "            print(f\"\\n{chain_name.upper()}: No successful queries\")\n",
        "\n",
        "analyze_performance(performance_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Advanced Features and Optimizations\n",
        "\n",
        "Let's explore some advanced features and optimization techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced similarity search with filtering\n",
        "def advanced_similarity_search():\n",
        "    query = \"What is vector search?\"\n",
        "    \n",
        "    print(\"🔍 Advanced Similarity Search Comparison\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Test different search parameters\n",
        "    search_params = [\n",
        "        {\"k\": 1, \"description\": \"Top 1 result\"},\n",
        "        {\"k\": 3, \"description\": \"Top 3 results\"},\n",
        "        {\"k\": 5, \"description\": \"Top 5 results\"}\n",
        "    ]\n",
        "    \n",
        "    for vectorstore_name, vectorstore in [(\"FAISS\", faiss_vectorstore), (\"ChromaDB\", chroma_vectorstore)]:\n",
        "        if vectorstore:\n",
        "            print(f\"\\n{vectorstore_name} Results:\")\n",
        "            for params in search_params:\n",
        "                try:\n",
        "                    results = vectorstore.similarity_search(query, k=params[\"k\"])\n",
        "                    print(f\"  {params['description']}: {len(results)} documents\")\n",
        "                    if results:\n",
        "                        print(f\"    Most similar: {results[0].page_content[:100]}...\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error: {e}\")\n",
        "\n",
        "advanced_similarity_search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Similarity search with scores\n",
        "def similarity_search_with_scores():\n",
        "    query = \"How does semantic search work?\"\n",
        "    \n",
        "    print(\"\\n📈 Similarity Search with Scores\")\n",
        "    print(\"=\" * 45)\n",
        "    \n",
        "    for vectorstore_name, vectorstore in [(\"FAISS\", faiss_vectorstore)]:\n",
        "        if vectorstore:\n",
        "            try:\n",
        "                # Get similarity search with scores\n",
        "                results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
        "                \n",
        "                print(f\"\\n{vectorstore_name} Results with Scores:\")\n",
        "                for i, (doc, score) in enumerate(results_with_scores, 1):\n",
        "                    print(f\"  {i}. Score: {score:.4f}\")\n",
        "                    print(f\"     Content: {doc.page_content[:80]}...\")\n",
        "                    print()\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "\n",
        "similarity_search_with_scores()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Interactive RAG Demo\n",
        "\n",
        "Create an interactive function to test the RAG system with custom queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_rag_demo():\n",
        "    \"\"\"\n",
        "    Interactive demo function for testing RAG with different vector databases\n",
        "    \"\"\"\n",
        "    print(\"🤖 Interactive RAG Demo\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"Available databases:\", list(rag_chains.keys()))\n",
        "    \n",
        "    # Example queries for demonstration\n",
        "    demo_queries = [\n",
        "        \"What are the advantages of cloud vector databases?\",\n",
        "        \"Explain the difference between FAISS and Pinecone\",\n",
        "        \"How do embeddings work in vector search?\"\n",
        "    ]\n",
        "    \n",
        "    for query in demo_queries:\n",
        "        print(f\"\\n❓ Query: {query}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        for db_name, chain in rag_chains.items():\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = chain({\"query\": query})\n",
        "                end_time = time.time()\n",
        "                \n",
        "                print(f\"\\n🔹 {db_name.upper()} ({end_time-start_time:.2f}s):\")\n",
        "                print(f\"   {response['result'][:150]}...\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\n🔹 {db_name.upper()}: Error - {e}\")\n",
        "\n",
        "# Run the interactive demo\n",
        "interactive_rag_demo()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}